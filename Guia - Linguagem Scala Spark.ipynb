{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SPARK & SCALA\n",
    "\n",
    "![Spark Scaala](https://d6vdma9166ldh.cloudfront.net/media/images/fce9b129-21e4-42d7-8094-1b58e6567b31.jpg)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para instalar o Scala Spark no jupyter notebook https://www.techentice.com/how-to-setup-jupyter-notebook-to-run-scala-and-spark/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//conda install -c conda-forge spylon-kernel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para usar direto no cmd windows basta chamar o ``spark-shell``\n",
    "\n",
    "Neste script foi utilizado:           \n",
    "spark 3.1.2               \n",
    "Scala 2.12.10              \n",
    "Java 64-Bit Server VM, Java 1.8.0_301       "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "Bases de dados para download:          \r\n",
    " https://1drv.ms/u/s!ApNnZoQvHI2wkZkBER3BwGqEXMzgJw?e=3nuosn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para utilizar no Visual Studio Code instalar extensões \n",
    "- Interactive Scala\n",
    "- Scala(Metals)\n",
    "- Scala Language Server\n",
    "- Scala Syntax(official)\n",
    "- Spark & Hive Tools (e é obrigatório ativar o spylon-kernel)\n",
    "\n",
    "Obs: desativar o spylon-kernel [Spark & Hive Tools] quando não for utiliza-lo pois ele da erro no python no VSCode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Scala - Introdução             \n",
    "\n",
    "Scala é uma linguagem de programação multiparadigma moderna projetada para expressar padrões de programação comuns de maneira concisa, elegante e segura. Scala foi criado por Martin Odersky e ele lançou a primeira versão em 2003. Scala integra suavemente os recursos de linguagens funcionais e orientadas a objetos. Este tutorial explica os fundamentos do Scala de uma maneira simples e fácil de ler.\n",
    "\n",
    "**Público**                      \n",
    "Este tutorial foi preparado para iniciantes para ajudá-los a entender os fundamentos do Scala em etapas simples e fáceis. Depois de concluir este tutorial, você se encontrará com um nível moderado de especialização no uso do Scala, de onde poderá avançar para os próximos níveis.\n",
    "\n",
    "**Pré-requisitos**                 \n",
    "A programação Scala é baseada em Java, portanto, se você conhece a sintaxe Java, é muito fácil aprender Scala. Além disso, se você não tem experiência em Java, mas conhece qualquer outra linguagem de programação como C, C ++ ou Python, isso também ajudará a compreender os conceitos de Scala muito rapidamente.\n",
    "\n",
    "###  Visão Geral               \n",
    "Scala, abreviação de linguagem escalável, é compilado para rodar na máquina virtual Java. Muitas empresas existentes, que dependem de Java para aplicativos críticos de negócios, estão recorrendo ao Scala para aumentar sua produtividade de desenvolvimento, escalabilidade de aplicativos e confiabilidade geral.\n",
    "\n",
    "Apresentamos aqui alguns pontos que tornam o Scala a primeira escolha dos desenvolvedores de aplicativos.\n",
    "\n",
    "**Scala é orientado a objetos***               \n",
    "Scala é uma linguagem puramente orientada a objetos, no sentido de que todo valor é um objeto. Tipos e comportamento de objetos são descritos por classes e características que serão explicadas nos capítulos subsequentes.\n",
    "\n",
    "As classes são estendidas por subclasses e um mecanismo de composição flexível baseado em mixin como um substituto limpo para herança múltipla.\n",
    "\n",
    "**Scala é funcional**               \n",
    "Scala também é uma linguagem funcional no sentido de que cada função é um valor e cada valor é um objeto, portanto, em última análise, cada função é um objeto.         \n",
    "\n",
    "Scala fornece uma sintaxe leve para definir funções anônimas, suporta funções de ordem superior, permite que funções sejam aninhadas e suporta currying. Esses conceitos serão explicados nos capítulos subsequentes.\n",
    "\n",
    "**Scala é digitado estaticamente**       \n",
    "Scala, ao contrário de algumas das outras linguagens tipadas estaticamente (C, Pascal, Rust, etc.), não espera que você forneça informações de tipo redundantes. Você não precisa especificar um tipo na maioria dos casos e certamente não precisa repeti-lo.\n",
    "\n",
    "**Scala é executado no JVM**           \n",
    "Scala é compilado em Java Byte Code que é executado pela Java Virtual Machine (JVM). Isso significa que Scala e Java têm uma plataforma de tempo de execução comum. Você pode facilmente mudar de Java para Scala.\n",
    "\n",
    "O compilador Scala compila seu código Scala em Java Byte Code, que pode então ser executado pelo comando 'scala'. O comando 'scala' é semelhante ao comando java , pois executa o código Scala compilado.\n",
    "\n",
    "**Scala pode executar código Java**             \n",
    "Scala permite que você use todas as classes do Java SDK e também suas próprias classes Java personalizadas ou seus projetos de código aberto Java favoritos.\n",
    "\n",
    "Scala pode fazer processamento simultâneo e sincronizado\n",
    "Scala permite que você expresse padrões gerais de programação de uma maneira eficaz. Ele reduz o número de linhas e ajuda o programador a codificar de forma segura. Ele permite que você escreva códigos de maneira imutável, o que facilita a aplicação de simultaneidade e paralelismo (Sincronizar).\n",
    "\n",
    "**Scala vs Java**             \n",
    "Scala possui um conjunto de recursos completamente diferente do Java. Alguns destes são -\n",
    "\n",
    "- Todos os tipos são objetos              \n",
    "- Inferência de tipo               \n",
    "- Funções Aninhadas              \n",
    "- Funções são objetos                \n",
    "- Suporte a idioma específico de domínio (DSL)                \n",
    "- Características                  \n",
    "- Fechamentos                     \n",
    "- Suporte de simultaneidade inspirado em Erlang     \n",
    "\n",
    "**Scala Web Frameworks**                \n",
    "Scala está sendo usado em todos os lugares e de forma importante em aplicativos da Web corporativos. Você pode verificar algumas das estruturas da web Scala mais populares                    \n",
    "- The Lift Framework\n",
    "- A estrutura do Play\n",
    "- A estrutura Bowler\n",
    "\n",
    "## Scala - Configuração do Ambiente \n",
    "\n",
    "O Scala pode ser instalado em qualquer sistema baseado em UNIX ou Windows. Antes de iniciar a instalação do Scala em sua máquina, você deve ter o Java 1.8 ou superior instalado em seu computador.\n",
    "\n",
    "Siga as etapas abaixo para instalar o Scala.\n",
    "\n",
    "### Instalando o Scala e Spark no Windons\n",
    "[Link do Video no Youtube](https://www.youtube.com/watch?v=7ht0tigbJXk)\n",
    "\n",
    "\n",
    "**adicionar mais informações**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scala - sintaxe básica\n",
    "\n",
    "Se você tiver um bom conhecimento de Java, será muito fácil aprender Scala. A maior diferença sintática entre Scala e Java é que o ';' o caractere de fim de linha é opcional.\n",
    "\n",
    "Quando consideramos um programa Scala, ele pode ser definido como uma coleção de objetos que se comunicam por meio da chamada dos métodos uns dos outros. Vamos agora dar uma olhada rápida no que significam as variáveis de classe, objeto, métodos e instância.          \n",
    "\n",
    "- Objeto - os objetos têm estados e comportamentos. Um objeto é uma instância de uma classe. Exemplo - Um cão tem estados - cor, nome, raça e também comportamentos - abanando, latindo e comendo.\n",
    "\n",
    "- Classe - uma classe pode ser definida como um modelo / projeto que descreve os comportamentos / estados relacionados à classe.\n",
    "\n",
    "- Métodos - Um método é basicamente um comportamento. Uma classe pode conter muitos métodos. É nos métodos onde as lógicas são escritas, os dados são manipulados e todas as ações são executadas.\n",
    "\n",
    "- Campos - Cada objeto tem seu conjunto único de variáveis de instância, que são chamadas de campos. O estado de um objeto é criado pelos valores atribuídos a esses campos.\n",
    "\n",
    "- Fechamento - Um fechamento é uma função, cujo valor de retorno depende do valor de uma ou mais variáveis declaradas fora desta função.\n",
    "\n",
    "- Traços - um traço encapsula definições de métodos e campos, que podem então ser reutilizados misturando-os em classes. Traits são usados para definir tipos de objetos, especificando a assinatura dos métodos suportados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala -  Primeiros Passos\n",
    "\n",
    "Podemos executar um programa Scala em dois modos: um é o modo interativo e outro é o modo script .\n",
    "\n",
    "- **Modo Interativo** utilizando o prompt de comando e use o seguinte comando para abrir o Scala.\n",
    "- **Modo de Script** Use as instruções a seguir para escrever um programa Scala no modo de script.             \n",
    "No modo script podemos utilizar o Jupyter notebook, python, R e outros.\n",
    "\n",
    "### Sintaxe Básica\n",
    "A seguir estão as sintaxes básicas e convenções de codificação na programação Scala.\n",
    "\n",
    "- Sensibilidade a maiúsculas e minúsculas - Scala diferencia maiúsculas de minúsculas, o que significa que os identificadores Hello e hello teriam significados diferentes em Scala.\n",
    "\n",
    "- Nomes de classes - para todos os nomes de classes, a primeira letra deve estar em maiúsculas. Se várias palavras forem usadas para formar o nome da classe, a primeira letra de cada palavra interna deve estar em maiúscula.\n",
    "\n",
    "     **Exemplo** classe MyFirstScalaClass.                   \n",
    "\n",
    "- Nomes de métodos - todos os nomes de métodos devem começar com uma letra minúscula. Se várias palavras forem usadas para formar o nome do método, a primeira letra de cada palavra interna deve estar em maiúscula.\n",
    "\n",
    "     **Exemplo** def myMethodName()                            \n",
    "\n",
    "- Nome do arquivo do programa - o nome do arquivo do programa deve corresponder exatamente ao nome do objeto. Ao salvar o arquivo, você deve salvá-lo usando o nome do objeto (Lembre-se de que Scala diferencia maiúsculas de minúsculas) e acrescentar ' .scala ' ao final do nome. (Se o nome do arquivo e o nome do objeto não corresponderem, seu programa não será compilado).\n",
    "\n",
    "     **Exemplo** suponha que 'HelloWorld' seja o nome do objeto. Em seguida, o arquivo deve ser salvo como 'HelloWorld.scala'.                                      \n",
    "\n",
    "- def main(args: Array [String]) O processamento do programa Scala começa a partir do método main(), que é uma parte obrigatória de todo Programa Scala.\n",
    "\n",
    "### Identificadores Scala      \n",
    "Todos os componentes do Scala requerem nomes. Os nomes usados para objetos, classes, variáveis e métodos são chamados de identificadores. Uma palavra-chave não pode ser usada como identificador e os identificadores diferenciam maiúsculas de minúsculas. Scala oferece suporte a quatro tipos de identificadores.\n",
    "\n",
    "**Identificadores Alfanuméricos**               \n",
    "Um identificador alfanumérico começa com uma letra ou sublinhado, que pode ser seguido por outras letras, dígitos ou sublinhados. O caractere '$' é uma palavra-chave reservada no Scala e não deve ser usado em identificadores.\n",
    "\n",
    "A seguir estão os identificadores alfanuméricos legais:\n",
    "\n",
    "- age, salary, _value,  __1_value\n",
    "\n",
    "A seguir estão os identificadores ilegais:\n",
    "\n",
    "- $salary, 123abc, -salary\n",
    "\n",
    "\n",
    "### Identificadores de Operador\n",
    "\n",
    "Um identificador de operador consiste em um ou mais caracteres de operador. Os caracteres do operador são caracteres ASCII imprimíveis, como +,:,?, ~ Ou #.\n",
    "\n",
    "A seguir estão os identificadores legais de operador:\n",
    "\n",
    "- + ++ ::: <?> :>\n",
    "\n",
    "O compilador Scala \"mangle\" internamente os identificadores de operador para transformá-los em identificadores Java legais com `$` caracteres incorporados. Por exemplo, o identificador: -> seria representado internamente como `$` dois pontos `$` menos `$` maior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identificadores mistos               \n",
    "Um identificador misto consiste em um identificador alfanumérico, que é seguido por um sublinhado e um identificador de operador.\n",
    "\n",
    "A seguir estão os identificadores mistos legais                   \n",
    "\n",
    "- unary_+,  myvar_=\n",
    "    \n",
    "Aqui, unário_+ usado como um nome de método define um operador unário + e myvar_ = usado como nome de método define um operador de atribuição (sobrecarga de operador).\n",
    "\n",
    "Identificadores literais\n",
    "Um identificador literal é uma string arbitrária entre marcas de volta (`...`).\n",
    "\n",
    "A seguir estão os identificadores literais legais\n",
    "\n",
    "- `x` `<clinit>` `yield`\n",
    "\n",
    "**Palavras-chave Scala**           \n",
    "\n",
    "A lista a seguir mostra as palavras reservadas em Scala. Essas palavras reservadas não podem ser usadas como constantes ou variáveis ou quaisquer outros nomes de identificador.\n",
    "\n",
    "- abstract, case, catch, class, def, do, else ,extends, false, final, finally, for, forSome, if, implicit, import, lazy, match\tnew, Null, object, override, package, private, protected, return, sealed, super ,this, throw, trait, Try, true, type, val, Var, while, with, yield, -, :, =, =>, <-, <:, <%, >:, #, @\t\t\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comentários no Scala            \n",
    "\n",
    "Scala oferece suporte a comentários de uma e de várias linhas muito semelhantes a Java. Os comentários de várias linhas podem ser aninhados, mas precisam ser aninhados corretamente. Todos os caracteres disponíveis dentro de qualquer comentário são ignorados pelo compilador Scala."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "object HelloWorld {\n",
    "   /* Este é meu primeiro programa Java.  \n",
    "    * Ele ira printar 'Hello World' como saida\n",
    "    * Este é um exemplo de como fazer vários comentarios encadeados\n",
    "    */\n",
    "   def main(args: Array[String]) {\n",
    "      // Print Hello World\n",
    "      // Este é um exemplo de como fazer comentário em apenas uma linha\n",
    "      println(\"Hello, world!\") \n",
    "   }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Linhas em branco e espaços em branco                            \n",
    "\n",
    "Uma linha contendo apenas espaços em branco, possivelmente com um comentário, é conhecida como linha em branco e o Scala a ignora totalmente. Os tokens podem ser separados por caracteres de espaço em branco e / ou comentários.\n",
    "\n",
    "### Caracteres de Newline                       \n",
    "\n",
    "Scala é uma linguagem orientada por linha em que as instruções podem ser encerradas por ponto-e-vírgula (;) ou novas linhas. Um ponto-e-vírgula no final de uma instrução geralmente é opcional. Você pode digitar um se quiser, mas não precisa se a instrução aparecer sozinha em uma única linha. Por outro lado, um ponto e vírgula é necessário se você escrever várias instruções em uma única linha. A sintaxe abaixo é o uso de várias instruções."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val s = \"hello\"; println(s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pacotes Scala                 \n",
    "\n",
    "Um pacote é um módulo de código nomeado. Por exemplo, o pacote do utilitário Lift é net.liftweb.util. A declaração do pacote é a primeira linha sem comentários no arquivo de origem da seguinte forma                 \n",
    "\n",
    "- package com.liftcode.stuff               \n",
    "\n",
    "Os pacotes Scala podem ser importados para que possam ser referenciados no escopo de compilação atual. A instrução a seguir importa o conteúdo do pacote scala.xml                     \n",
    "\n",
    "- import scala.xml._              \n",
    "\n",
    "Você pode importar uma única classe e objeto, por exemplo, HashMap do pacote scala.collection.mutable                      \n",
    "\n",
    "- import scala.collection.mutable.HashMap             \n",
    "\n",
    "Você pode importar mais de uma classe ou objeto de um único pacote, por exemplo, TreeMap e TreeSet do pacote scala.collection.immutable                      \n",
    "\n",
    "- import scala.collection.immutable.{TreeMap, TreeSet}\n",
    "\n",
    "### Aplicar Dinâmica\n",
    "\n",
    "Um traço de marcador que permite invocações dinâmicas. As instâncias x desta característica permitem invocações de método x.meth(args) para nomes de métodos arbitrários meth e args de listas de argumentos, bem como acessos a campos x.field para campos de nomes de campos arbitrários. Este recurso é apresentado no Scala-2.10.\n",
    "\n",
    "Se uma chamada não for nativamente suportada por x (ou seja, se a verificação de tipo falhar), ela será reescrita de acordo com as seguintes regras\n",
    "\n",
    "- foo.method(\"blah\") ~~> foo.applyDynamic(\"method\")(\"blah\")\n",
    "- foo.method(x = \"blah\") ~~> foo.applyDynamicNamed(\"method\")((\"x\", \"blah\"))\n",
    "- foo.method(x = 1, 2) ~~> foo.applyDynamicNamed(\"method\")((\"x\", 1), (\"\", 2))\n",
    "- foo.field ~~> foo.selectDynamic(\"field\")\n",
    "- foo.varia = 10 ~~> foo.updateDynamic(\"varia\")(10)\n",
    "- foo.arr(10) = 13 ~~> foo.selectDynamic(\"arr\").update(10, 13)\n",
    "- foo.arr(10) ~~> foo.applyDynamic(\"arr\")(10)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Print Hello World\n",
    "println(\"Hello World\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - Tipos de Dados                   \n",
    "\n",
    "Scala tem todos os mesmos tipos de dados que Java, com a mesma pegada de memória e precisão. A seguir está a tabela com detalhes sobre todos os tipos de dados disponíveis no Scala\n",
    "\n",
    "- Byte: Valor com sinal de 8 bits. Varia de -128 a 127\n",
    "- Short: Valor com sinal de 16 bits. Faixa -32768 a 32767\n",
    "- Int: Valor com sinal de 32 bits. Faixa -2147483648 a 2147483647\n",
    "- Long: Valor com sinal de 64 bits. -9223372036854775808 a 9223372036854775807\n",
    "- Float: Flutuador de precisão simples IEEE 754 de 32 bits\n",
    "- Double: Flutuador de dupla precisão IEEE 754 de 64 bits\n",
    "- Char: Caractere Unicode não assinado de 16 bits. Varia de U + 0000 a U + FFFF\n",
    "- String: Uma sequência de caracteres\n",
    "- Boolean: O literal verdadeiro (TRUE) ou o literal falso (FALSE)\n",
    "- Unit: Não corresponde a nenhum valor\n",
    "- Null: referência nula ou vazia\n",
    "- Nothing: O subtipo de todos os outros tipos; não inclui valores\n",
    "- Any: O supertipo de qualquer tipo; qualquer objeto é do tipo Qualquer\n",
    "- AnyRef: O supertipo de qualquer tipo de referência        \n",
    "\n",
    "Todos os tipos de dados listados acima são objetos. Não existem tipos primitivos como em Java. Isso significa que você pode chamar métodos em um Int, Long, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Gerar um número do tipo interge (inteiro)\n",
    "100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Gerar um número do tipo double (continuo)\n",
    "2.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Operadores\n",
    "\n",
    "Um operador é um símbolo que diz ao compilador para realizar manipulações matemáticas ou lógicas específicas. Scala é rico em operadores integrados e fornece os seguintes tipos de operadores -\n",
    "\n",
    "- Operadores aritméticos\n",
    "- Operadores relacionais\n",
    "- Operadores lógicos\n",
    "- Operadores bit a bit\n",
    "- Operadores de atribuição\n",
    "\n",
    "Este capítulo examinará os operadores aritméticos, relacionais, lógicos, de atribuição de bits e outros, um por um."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores aritméticos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//soma\n",
    "1+1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//subtração\n",
    "2-1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Multiplicação\n",
    "2*2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Divisão\n",
    "8/2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Potência\n",
    "math.pow(4,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//soma e multiplicação\n",
    "1+2*3+4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//soma e multiplicação\n",
    "(1+2)*(3+4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Converter 3 pés em metros (onde 1 pé = 0.3048 metros)\n",
    "3*0.3048"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores relacionais\n",
    "\n",
    "Os seguintes operadores relacionais são suportados pela linguagem Scala. Por exemplo, vamos supor que a variável A detém 10 e a variável B detém 20, então:\n",
    "\n",
    "- `==` Verifica se os valores dos dois operandos são iguais ou não, se sim então a condição torna-se verdadeira.\t(A == B) não é verdade.   \n",
    "\n",
    "- `!=`\tVerifica se os valores de dois operandos são iguais ou não, se os valores não são iguais, a condição torna-se verdadeira. (A! = B) é verdade.    \n",
    "\n",
    "- `>`\tVerifica se o valor do operando esquerdo é maior que o valor do operando direito, se sim, a condição torna-se verdadeira.\t(A > B) não é verdade.\n",
    "\n",
    "- `<`\tVerifica se o valor do operando esquerdo é menor que o valor do operando direito, se sim, a condição torna-se verdadeira.\t(A < B) é verdade.\n",
    "\n",
    "- `>=`\tVerifica se o valor do operando esquerdo é maior ou igual ao valor do operando direito; em caso afirmativo, a condição torna-se verdadeira.\t(A>= B) não é verdade.\n",
    "\n",
    "- `<=`\tVerifica se o valor do operando esquerdo é menor ou igual ao valor do operando direito, se sim então a condição torna-se verdadeira.\t(A <= B) é verdadeiro."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores lógicos\n",
    "\n",
    "Os seguintes operadores lógicos são suportados pela linguagem Scala. Por exemplo, suponha que a variável A detém 1 e a variável B detém 0, então:\n",
    "\n",
    "- `&&`\tÉ chamado de operador lógico AND. Se ambos os operandos forem diferentes de zero, a condição torna-se verdadeira. (A && B) é falso.\n",
    "\n",
    "- `||` É denominado Operador OR lógico. Se qualquer um dos dois operandos for diferente de zero, a condição se torna verdadeira. (A || B) é verdade.\n",
    "\n",
    "- `!` É denominado Operador NOT lógico. Use para reverter o estado lógico de seu operando. Se uma condição for verdadeira, o operador lógico NOT tornará falsa.\t! (A && B) é verdade."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores bit a bit\n",
    "\n",
    "O operador bit a bit funciona em bits e executa a operação bit a bit. As tabelas de verdade para &, | e ^ são as seguintes:\n",
    "\n",
    "Suponha que A = 60; e B = 13; agora em formato binário serão os seguintes                            \n",
    "\n",
    "A = 0011 1100                 \n",
    "B = 0000 1101                     \n",
    "*--------------                  \n",
    "A&B = 0000 1100                      \n",
    "A|B = 0011 1101                 \n",
    "A^B = 0011 0001                  \n",
    "~A = 1100 0011                      \n",
    "\n",
    "Os operadores bit a bit suportados pela linguagem Scala estão listados na tabela a seguir. Suponha que a variável A tenha 60 e a variável B tenha 13, então:\n",
    "\n",
    "- `E`\tO operador Binário AND copia um bit para o resultado se ele existir em ambos os operandos.\t(A e B) dará 12, que é 0000 1100\n",
    "\n",
    "- `|`\tO operador binário OR copia um bit se ele existir em qualquer um dos operandos.\t(A | B) dará 61, que é 0011 1101\n",
    "\n",
    "- `^`\tO operador binário XOR copia o bit se estiver definido em um operando, mas não em ambos.\t(A ^ B) dará 49, que é 0011 0001\n",
    "\n",
    "- `~`\tO operador de complemento binários é unário e tem o efeito de 'inverter' bits.\t(~ A) resultará em -61, que é 1100 0011 na forma de complemento de 2 devido a um número binário com sinal.\n",
    "\n",
    "- `<<`\tOperador binário de deslocamento à esquerda. As posições dos bits do valor dos operandos à esquerda são movidas para a esquerda pelo número de bits especificado pelo operando à direita.\tUm << 2 dará 240, que é 1111 0000\n",
    "\n",
    "- `>>`\tOperador binário de deslocamento à direita. As posições dos bits do valor do operando esquerdo são movidas para a direita pelo número de bits especificado pelo operando direito.\tUm >> 2 dará 15, que é 1111\n",
    "\n",
    "- `>>>`\tDesloque o operador de preenchimento de zero para a direita. O valor dos operandos à esquerda é movido para a direita pelo número de bits especificado pelo operando à direita e os valores deslocados são preenchidos com zeros.\tUm >>> 2 dará 15, que é 0000 1111\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Operadores de atribuição\n",
    "\n",
    "Existem os seguintes operadores de atribuição suportados pela linguagem Scala:\n",
    "\n",
    "- `=`\tOperador de atribuição simples, atribui valores de operandos do lado direito para operando do lado esquerdo\tC = A + B irá atribuir o valor de A + B em C\n",
    "\n",
    "- `+=`\tAdicionar operador de atribuição AND, adiciona o operando direito ao operando esquerdo e atribui o resultado ao operando esquerdo\tC + = A é equivalente a C = C + A\n",
    "\n",
    "- `-=`\tSubtrai o operador de atribuição AND, subtrai o operando direito do operando esquerdo e atribui o resultado ao operando esquerdo\tC - = A é equivalente a C = C - A\n",
    "\n",
    "- `*=`\tMultiplicar E operador de atribuição, multiplica o operando direito com o operando esquerdo e atribui o resultado ao operando esquerdo\tC * = A é equivalente a C = C * A\n",
    "\n",
    "- `/=`\tOperador de atribuição Dividir AND, divide o operando esquerdo com o operando direito e atribui o resultado ao operando esquerdo\tC / = A é equivalente a C = C / A\n",
    "\n",
    "- `%=`\tMódulo E operador de atribuição, leva o módulo usando dois operandos e atribui o resultado ao operando esquerdo\tC% = A é equivalente a C = C% A\n",
    "\n",
    "- `<<=`\tDeslocamento à esquerda E operador de atribuição\tC << = 2 é igual a C = C << 2\n",
    "\n",
    "- `>>=`\tDeslocamento à direita E operador de atribuição\tC >> = 2 é igual a C = C >> 2\n",
    "\n",
    "- `&=`\tOperador de atribuição AND bit a bit\tC & = 2 é igual a C = C & 2\n",
    "\n",
    "- `^=`\tOR exclusivo bit a bit e operador de atribuição\tC ^ = 2 é igual a C = C ^ 2\n",
    "\n",
    "- `|=`\tOR inclusivo bit a bit e operador de atribuição\tC | = 2 é igual a C = C | 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Precedência de operadores em Scala\n",
    "\n",
    "A precedência do operador determina o agrupamento de termos em uma expressão. Isso afeta como uma expressão é avaliada. Certos operadores têm precedência mais alta do que outros; por exemplo, o operador de multiplicação tem precedência mais alta do que o operador de adição\n",
    "\n",
    "Por exemplo, x = 7 + 3 * 2; aqui, x é atribuído a 13, não 20, porque o operador * tem precedência mais alta do que +, então ele primeiro é multiplicado por 3 * 2 e, em seguida, é adicionado a 7.\n",
    "\n",
    "Dê uma olhada na tabela a seguir. Os operadores com a precedência mais alta aparecem na parte superior da tabela e aqueles com a precedência mais baixa aparecem na parte inferior. Em uma expressão, os operadores de precedência mais alta serão avaliados primeiro.\n",
    "\n",
    "- Postfix ()[] \tDa esquerda para direita\n",
    "- Unário ! ~\tDireita para esquerda\n",
    "- Multiplicativo * / %\tDa esquerda para direita\n",
    "- Aditivo + -\tDa esquerda para direita\n",
    "- Mudançan >> >>> << Da esquerda para direita\n",
    "- Relacional >> = <<= Da esquerda para direita\n",
    "- Igualdade\t== != Da esquerda para direita\n",
    "- E bit a bit E Da esquerda para direita\n",
    "- XOR bit a bit\t^ Da esquerda para direita\n",
    "- OR bit a bit | Da esquerda para direita\n",
    "- E lógico && Da esquerda para direita\n",
    "- OR lógico\t|| Da esquerda para direita\n",
    "- Atribuição = + = - = * = / =% = >> = << = & = ^ = | =\tDireita para esquerda\n",
    "- Parágrafo, Da esquerda para direita"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemplos - Booleanos e operadores de comparação"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Boleanos (TRUE, FALSE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Boleanos\n",
    "true"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Boleanos\n",
    "false"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Operadores de comparação (<, <=, >, >=, ==, !=, %)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (<) menor\n",
    "1>2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (>=) maior ou igual\n",
    "1>=2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (==) igual\n",
    "2==2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (!=) diferente\n",
    "2!=2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (%) restante da divisão\n",
    "2%2\n",
    "//também é uma forma de verificar se um número é par, pois se ele é par o resto é zero"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Comparação (%) restante da divisão\n",
    "5%2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemplos - String e Expressão Regular"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Imprimindo uma String (obs: sempre com aspas duplas)\n",
    "println(\"Hello World\")\n",
    "print(\"Hello World\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Concatenar duas strings\n",
    "val Despedida = \"good\"+\"bye\"\n",
    "\n",
    "//Ou\n",
    "\"good\"+\"bye\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Repetir 5 vezes a mesma string\n",
    "\"dance\"*5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Gerar string\n",
    "val st=\"Hello\"\n",
    "\n",
    "//Ver total de letras \n",
    "st.length"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Concatenar chamando uma string depois de outra\n",
    "val name=\"Jose\"\n",
    "\n",
    "//chamando\n",
    "val cumprimento = s\"Oi ${name}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Concatenar chamando uma string depois de outra 2\n",
    "val cumprimento = s\"Oi $name\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Concatenar chamando uma string depois de outra 3\n",
    "val cumprimento = f\"Oi $name\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Operações com string e Slice\n",
    "val st= \"Esta e uma string longa\"\n",
    "st"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Pedir a primeira letra\n",
    "st.charAt(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Pedir a terceira letra\n",
    "st.charAt(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Pedir a posição da letra m\n",
    "st.indexOf(\"m\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//slice\n",
    "st slice(0,4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//slice\n",
    "st slice(11,17)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Verificar se uma string é igual a outra\n",
    "val string_1=\"banana acabate uva\"\n",
    "val string_2=\"banana acabate uva pera\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Verificando string_1\n",
    "string_1 matches \"banana acabate uva\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Verificando string_2\n",
    "string_2 matches \"banana acabate uva\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Verificando string_1 string_2\n",
    "string_1==string_2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Verificando se string2 contêm pera\n",
    "string_2 contains \"pera\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - Variáveis\n",
    "\n",
    "As variáveis nada mais são do que locais de memória reservados para armazenar valores. Isso significa que, ao criar uma variável, você reserva algum espaço na memória.\n",
    "\n",
    "Com base no tipo de dados de uma variável, o compilador aloca memória e decide o que pode ser armazenado na memória reservada. Portanto, ao atribuir diferentes tipos de dados às variáveis, você pode armazenar inteiros, decimais ou caracteres nessas variáveis.\n",
    "\n",
    "### Declaração de Variável\n",
    "\n",
    "Scala tem uma sintaxe diferente para declarar variáveis. Eles podem ser definidos como um valor, ou seja, uma constante ou uma variável. Aqui, myVar é declarado usando a palavra-chave var. É uma variável que pode mudar de valor e é chamada de variável mutável . A seguir está a sintaxe para definir uma variável usando a palavra-chave var\n",
    "\n",
    "Sintaxe\n",
    "- var myVar : String = \"Foo\"\n",
    "\n",
    "Aqui, myVal é declarado usando a palavra-chave val. Isso significa que é uma variável que não pode ser alterada e é chamada de variável imutável . A seguir está a sintaxe para definir uma variável usando a palavra-chave val\n",
    "\n",
    "Sintaxe\n",
    "- val myVal : String = \"Foo\"\n",
    "\n",
    "### Tipos de dados variáveis\n",
    "\n",
    "O tipo de uma variável é especificado após o nome da variável e antes do sinal de igual. Você pode definir qualquer tipo de variável Scala mencionando seu tipo de dados da seguinte forma\n",
    "\n",
    "Sintaxe\n",
    "- val or val VariableName : DataType = [Initial Value]\n",
    "\n",
    "Se você não atribuir nenhum valor inicial a uma variável, ele é válido da seguinte forma\n",
    "\n",
    "Sintaxe\n",
    "- var myVar :Int;\n",
    "- val myVal :String;\n",
    "\n",
    "### Inferência de tipo de variável\n",
    "\n",
    "Quando você atribui um valor inicial a uma variável, o compilador Scala pode descobrir o tipo da variável com base no valor atribuído a ela. Isso é chamado de inferência de tipo de variável. Portanto, você pode escrever essas declarações de variáveis assim\n",
    "\n",
    "Sintaxe\n",
    "- var myVar = 10;\n",
    "- val myVal = \"Hello, Scala!\";\n",
    "\n",
    "Aqui, por padrão, myVar será do tipo Int e myVal se tornará a variável do tipo String.\n",
    "\n",
    "### Múltiplas atribuições\n",
    "\n",
    "Scala oferece suporte a várias atribuições. Se um bloco de código ou método retornar uma tupla ( Tupla - contém uma coleção de objetos de diferentes tipos), a tupla pode ser atribuída a uma variável val. [ Nota - estudaremos tuplas nos capítulos subsequentes.]\n",
    "\n",
    "Sintaxe\n",
    "- val (myVar1: Int, myVar2: String) = Pair(40, \"Foo\")\n",
    "\n",
    "E a inferência de tipo acerta\n",
    "\n",
    "Sintaxe\n",
    "- val (myVar1, myVar2) = Pair(40, \"Foo\")\n",
    "\n",
    "### Escopo Variável\n",
    "\n",
    "As variáveis no Scala podem ter três escopos diferentes, dependendo do local onde estão sendo usadas. Eles podem existir como campos, como parâmetros de método e como variáveis locais. Abaixo estão os detalhes sobre cada tipo de escopo.\n",
    "\n",
    "### Campos\n",
    "\n",
    "Os campos são variáveis que pertencem a um objeto. Os campos são acessíveis de dentro de todos os métodos do objeto. Os campos também podem ser acessíveis fora do objeto, dependendo de quais modificadores de acesso o campo é declarado. Os campos de objeto podem ser tipos mutáveis e imutáveis e podem ser definidos usando var ou val .\n",
    "\n",
    "### Parâmetros do Método\n",
    "\n",
    "Os parâmetros do método são variáveis, usadas para passar o valor dentro de um método, quando o método é chamado. Os parâmetros do método são acessíveis apenas de dentro do método, mas os objetos passados podem ser acessíveis de fora, se você tiver uma referência ao objeto de fora do método. Os parâmetros do método são sempre imutáveis e são definidos pela palavra-chave val .\n",
    "\n",
    "### Local Variables\n",
    "\n",
    "Variáveis locais são variáveis declaradas dentro de um método. Variáveis locais são acessíveis apenas de dentro do método, mas os objetos que você cria podem escapar do método se você retorná-los do método. Variáveis locais podem ser do tipo mutável e imutável e podem ser definidas usando var ou val ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemplos - Valores e Variáveis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criar uma variável integer\n",
    "var myvar: Int = 10\n",
    "\n",
    "//Ver valor de myvar\n",
    "myvar"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Printar variável\n",
    "println(myvar)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criar um valor double\n",
    "val myval: Double = 2.5\n",
    "\n",
    "//Ver valor de myval\n",
    "myval"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Printar valor\n",
    "println(myval)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criar um valor de forma mais simples\n",
    "val c=12\n",
    "\n",
    "//Ver valor de c\n",
    "c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criar uma string\n",
    "val my_string= \"Hello\"\n",
    "\n",
    "//Ver my_string\n",
    "my_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criar uma string com caracter especial no nome ou espaço\n",
    "val `string nova`=\"teste\"\n",
    "\n",
    "//Ver\n",
    "`string nova`"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Scala - Tuplas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criando uma tupla\n",
    "val my_tupla_1 = (1,2.2,\"Hello\",23.2,true)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Criando uma tupla dentro de uma tupla\n",
    "val my_tupla = (1,2.2,\"Hello\",23.2,true,(2.5,10,false))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//Acessando o elemento 3 de uma tupla\n",
    "my_tupla_1._3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Scala -  Listas\n",
    "\n",
    "OBS: Assim como o Python o Scala é indexado em zero, ou seja, se quiser ver o primeiro elemento precisa pedir o elemento zero."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "//criar uma lista\n",
    "val Lista= List(2, 4, 6, 8, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Consultar elementos de uma lista\n",
    "Lista(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Consultar elementos de uma lista\n",
    "Lista(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o primeiro elemento\n",
    "Lista.head"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o utimo elemento\n",
    "Lista.tail"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar uma lista dentro de uma lista\n",
    "val Lista2= List(List(2, 4, 6), 8, 10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Consultar elementos de uma lista\n",
    "Lista2(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar uma lista dentro de uma lista\n",
    "val Lista3= List((\"a\",1), (\"b\",2), (\"c\",3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Consultar elementos de uma lista\n",
    "Lista3(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ordenar elementos dentro de uma lista\n",
    "val Lista4= List(99, 5, 37.9, 23, 10, 15, 2 , 3, 3, 58.9)\n",
    "Lista.sorted"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver tamanho de uma lista\n",
    "Lista4.size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o valor máximo de uma lista\n",
    "Lista4.max"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o valor minimo de uma lista\n",
    "Lista4.min"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o valor da soma de uma lista\n",
    "Lista4sta.sum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o valor da multiplicação dos valores de uma lista\n",
    "Lista4.product"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Dropar o primeiro elemento da lista\n",
    "Lista4.drop(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pegar três elemento da lista da direita para esquerda\n",
    "Lista4.takeRight(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Slice de uma lista\n",
    "val Lista5= List(1,2,3,4,5,6,7,8,9,10)\n",
    "Lista5 slice (0, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar uma Lista de 0 a 20\n",
    "val Lista6 = List.range(0,21)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar uma Lista de 0 a 20 com intervalo de 2 em 2 \n",
    "val Lista7 = List.range(0,21,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Scala -  Arrays\n",
    "\n",
    "Todos os exemplos das listas valem para os arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um array numerico\n",
    "val array_1 = Array(1,2,3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um array string\n",
    "val array_2 = Array(\"a\", \"b\", \"c\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar uma array de 0 a 20\n",
    "val array_3 = Array.range(0,21)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar uma array  de 0 a 20 com intervalo de 2 em 2\n",
    "val array_3 = Array.range(0,21,2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Scala - Sets\n",
    "\n",
    "sets = conjuntos\n",
    "\n",
    "- Os Sets não repetem elementos.\n",
    "- Os Sets por padrão são imutáveis, mas podem ser mutáveis se especificado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando um set vazio imutável\n",
    "val set_vazio = Set()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando um set imutável\n",
    "val set1 = Set(1,2,3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando um set imutável com elementos repetidos\n",
    "val set2 = Set(1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando um set vazio imutável\n",
    "val set3 = collection.mutable.Set(1,2,3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Adicionando um elemento ao um set mutável\n",
    "set3 +=4\n",
    "\n",
    "// Ou\n",
    "\n",
    "set3.add(5)\n",
    "\n",
    "// Ver\n",
    "print(set3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter uma lista para Set\n",
    "val Lista8 = List.range(0,21)\n",
    "Lista8.toSet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter uma Array para Set\n",
    "val Array4 = Array.range(0,21)\n",
    "Lista8.toSet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter uma Set para Lista\n",
    "val set3 = Set(1,2,3)\n",
    "set3.toList"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter uma Set para Array\n",
    "val set3 = Set(1,2,3)\n",
    "set3.toArray"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Scala - Maps\n",
    "\n",
    "Maps são como dicionários em Python ele tem uma chave e um valor.\n",
    "- Os Sets por padrão são imutáveis, mas podem ser mutáveis se especificado."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um map\n",
    "val Map_1 = Map((\"a\",1),(\"b\",2), (\"c\",3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Consultar um valor de um map pela chave\n",
    "Map_1(\"a\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pegando um elemento de um map pela chave\n",
    "Map_1 get \"a\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um map mutável\n",
    "val Map_2 = collection.mutable.Map((\"a\",1),(\"b\",2), (\"c\",3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Adicionar valores ao map\n",
    "Map_2 += (\"nova_chave\" -> 999)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver chaves de um map\n",
    "Map_2.keys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver valores de um map\n",
    "Map_2.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter um map para Array ou lista (Map_2.toList)\n",
    "Map_2.toArray"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - Operadores lógicos\n",
    "\n",
    "OBS: rodar no cmd via ``spark--shell``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um if\n",
    "if(true){\n",
    "    println(\"Vou printar se for verdade\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um if\n",
    "if(3 == 3){\n",
    "    println(\"3 é igual a 3\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um if\n",
    "val x=\"Hello\"\n",
    "\n",
    "if(x.endsWith(\"o\")){\n",
    "    println(\"O valor de x termina com o\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um if else\n",
    "val x=\"Ola\"\n",
    "\n",
    "if(x.endsWith(\"o\")){\n",
    "    println(\"O valor de x termina com o\")\n",
    "}else{\n",
    "    println(\"O valor de x não termina com o\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando print com AND\n",
    "println((1==2) && (2==2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando print com OR\n",
    "println((1==2) || (2==2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando print com NOT!\n",
    "println(!(1==1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - For Loops\n",
    "\n",
    "For = Para            \n",
    "Sintax básica\n",
    "\n",
    "``for (item <- sequência de interação){\n",
    "       fazer alguma coisa\n",
    "}``\n",
    "\n",
    "OBS: rodar no cmd via ``spark--shell``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Exemplo de for (irá três vezes Hello)\n",
    "for(item <- List(1,2,3)){\n",
    "    println(\"Hello\")  \n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Exemplo de for (irá printar 1 2 3)\n",
    "for(num <- List(1,2,3)){\n",
    "    println(num)  \n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Exemplo de for (irá printar quais numeros são par ou impar)\n",
    "for(numero <- Range(1,10)){\n",
    "    if(numero%2 == 0){\n",
    "        println(s\"$numero é par\")\n",
    "    }else{\n",
    "       println(s\"$numero é impar\") \n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Exemplo de for (irá printar quais numeros são par ou impar)\n",
    "val nomes = List(\"Anderson\", \"Andriana\", \"Calor\", \"Cintia\")\n",
    "\n",
    "for(nome <- nomes){\n",
    "    if(nome.startsWith(\"C\")){\n",
    "        println(s\"nome começa com C\")\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - While Loops\n",
    "\n",
    "While = Enquanto \n",
    "\n",
    "OBS: rodar no cmd via ``spark--shell``"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um Whille\n",
    "var x = 0\n",
    "\n",
    "while(x < 5){\n",
    "    println(s\"x esta correndo $x\")\n",
    "    println(\"x ainda é 5, adicionando 1 a x\")\n",
    "    x = x+1\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Montando um Whille\n",
    "import util.control.Breaks._ // importar módulo para Parar\n",
    "\n",
    "var y = 0\n",
    "\n",
    "while(y < 10){\n",
    "    println(s\"y esta correndo $y\")\n",
    "    println(\"y ainda e 10, adicionando 1 a y\")\n",
    "    y = y+1\n",
    "    if(y==7) break\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - Funções"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Definindo uma função para printar algo\n",
    "def simple(): Unit = {\n",
    "    println(\"printe simples\")\n",
    "}\n",
    "\n",
    "simple()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Definindo uma função para somar dois numeros inteiros\n",
    "def adder(num1:Int,num2:Int): Int = {\n",
    "    return num1 + num2\n",
    "}\n",
    "\n",
    "adder(4,5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Definindo uma função para printar Hello Jose\n",
    "def greetName(name:String): String={\n",
    "    return s\"Hello $name\"\n",
    "\n",
    "}\n",
    "val fullgreet = greetName(\"Jose\")\n",
    "println(fullgreet)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Checar se um numero é primo\n",
    "def isPrime(numcheck:Int): Boolean = {\n",
    "  for(n <- Range(2,numcheck)){\n",
    "    if(numcheck%n == 0){\n",
    "      return false\n",
    "    }\n",
    "    }\n",
    "    return true\n",
    "}\n",
    "\n",
    "println(isPrime(10))\n",
    "println(isPrime(23))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Usando coleções\n",
    "val numbers = List(1,2,3,7)\n",
    "def check(nums:List[Int]): List[Int]={\n",
    "  return nums\n",
    "}\n",
    "\n",
    "println(check(numbers))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Função para printar hello para um nome\n",
    "def quickgreet(name:String) = s\"Hello $name\"\n",
    "println(quickgreet(\"Sammy\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scala - Dataframe     \n",
    "\n",
    "Ver mais informações:[ https://spark.apache.org/docs/latest/sql-programming-guide.html ](https://spark.apache.org/docs/latest/sql-programming-guide.html)                 \n",
    "\n",
    "Spark SQL é um módulo Spark para processamento de dados estruturados. Ao contrário da API Spark RDD básica, as interfaces fornecidas pelo Spark SQL fornecem ao Spark mais informações sobre a estrutura dos dados e do cálculo que está sendo executado. Internamente, o Spark SQL usa essas informações extras para realizar otimizações extras. Existem várias maneiras de interagir com o Spark SQL, incluindo SQL e a API do conjunto de dados. Ao calcular um resultado, o mesmo mecanismo de execução é usado, independente de qual API / linguagem você está usando para expressar a computação. Essa unificação significa que os desenvolvedores podem alternar facilmente entre diferentes APIs com base em qual fornece a maneira mais natural de expressar uma determinada transformação.\n",
    "\n",
    "Todos os exemplos nesta página usam dados de amostra incluídos na distribuição do Spark e podem ser executados no spark-shell, pysparkshell ou sparkRshell.\n",
    "\n",
    "### *SQL*                         \n",
    "\n",
    "Um uso do Spark SQL é executar consultas SQL. O Spark SQL também pode ser usado para ler dados de uma instalação existente do Hive. Para obter mais informações sobre como configurar este recurso, consulte a seção Tabelas do Hive . Ao executar o SQL de dentro de outra linguagem de programação, os resultados serão retornados como um Dataset / DataFrame . Você também pode interagir com a interface SQL usando a linha de comando ou por meio de JDBC / ODBC .\n",
    "\n",
    "### *Conjuntos de dados e DataFrames*                   \n",
    "\n",
    "Um conjunto de dados é uma coleção distribuída de dados. Dataset é uma nova interface adicionada ao Spark 1.6 que fornece os benefícios de RDDs (tipagem forte, capacidade de usar funções lambda poderosas) com os benefícios do mecanismo de execução otimizado do Spark SQL. Um conjunto de dados pode ser construído a partir de objectos JVM e, em seguida, manipulada usando transformações funcionais ( map, flatMap, filter, etc.). A API Dataset está disponível em Scala e Java . Python não tem suporte para a API Dataset. Mas devido à natureza dinâmica do Python, muitos dos benefícios da API Dataset já estão disponíveis (ou seja, você pode acessar o campo de uma linha pelo nome naturalmente row.columnName). O caso de R é semelhante.\n",
    "\n",
    "Um DataFrame é um conjunto de dados organizado em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou a um quadro de dados em R / Python, mas com otimizações mais ricas sob o capô. Os DataFrames podem ser construídos a partir de uma ampla variedade de fontes , como: arquivos de dados estruturados, tabelas no Hive, bancos de dados externos ou RDDs existentes. A API trama de dados está disponível em Scala, Java, Python , e R . Em Scala e Java, um DataFrame é representado por um Dataset de Rows. Na API Scala , DataFrameé simplesmente um alias de tipo de Dataset[Row]. Enquanto, na API Java , os usuários precisam usar Dataset<Row>para representar a DataFrame.\n",
    "\n",
    "Ao longo deste documento, frequentemente nos referiremos aos conjuntos de dados Scala / Java de Rows como DataFrames."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Começando uma seção simples com Spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importando um CSV para o spark\n",
    "val df = spark.read.csv(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/CitiGroup2006_2008\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver as primeiras 5 linhas\n",
    "df.head(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Para ver um número especifico de linhas basta adicionar o número\n",
    "df.show(1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar esquema do df \n",
    "df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mandar printar as 10 primeiras linhas de um maneira mais legivel\n",
    "for(row <- df.head(10)){\n",
    "  println(row)\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ler um arquivo csv da maneira mais correta especificando o esquema ou seja, lendo o titulo das colunas\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").\n",
    "csv(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/CitiGroup2006_2008\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar esquema do df (agora temos o nome das variáveis)\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver nome das colunas\n",
    "df.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Estatisticas descritivas\n",
    "df.describe().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Correlação de Pearson \n",
    "df.select(corr(\"High\",\"Low\")).show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrando as linhas de uma coluna\n",
    "df.select(\"Volume\").show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrando as linhas de mais uma coluna selecionada\n",
    "df.select($\"Date\", $\"Close\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Estatisticas descritivas de uma coluna só\n",
    "df.select(\"Volume\").describe().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando uma nova coluna somando duas colunas existentes\n",
    "val df2 = df.withColumn(\"High_plus_low\",df(\"High\")+df(\"Low\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar esquema do df \n",
    "df2.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Renomeando Colunas (e selecionando mais algumas)\n",
    "df2.select(df2(\"High_plus_low\").as(\"HPL\"),df2(\"Close\")).show()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtrando dados\n",
    "\n",
    "Operações e funções úteis                \n",
    "http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\n",
    "\n",
    "[ https://spark.apache.org/docs/2.2.0/sql-programming-guide.html ](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html)   \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Esta importação é necessária para usar a $-notation\n",
    "import spark.implicits._"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pegando todas as linhas onde close é maior que 480 dolares\n",
    "df.filter($\"Close\" > 480).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Também pode usar notação SQL para filtrar do mesmo jeito\n",
    "df.filter(\"Close > 480\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Conte quantos tem close > 480 dolares\n",
    "df.filter($\"Close\">480).count()\n",
    "\n",
    "// Usando notação SQL\n",
    "//df.filter(\"Close > 480\").count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo um valor exato para variavel High\n",
    "df.filter($\"High\"===484.4).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// SQL para contar quantos tem um valor exato\n",
    "df.filter(\"High = 484.4\").count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar quantos tem close e High < 480 dolares\n",
    "df.filter($\"Close\"<480 && $\"High\"<480).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Contar quantos tem close e High < 480 dolares\n",
    "df.filter($\"Close\"<480 && $\"High\"<480).count()\n",
    "\n",
    "// OU\n",
    "df.filter(\"Close <480 AND High <480\").count()\n",
    "\n",
    "// Em SQL\n",
    "//df.filter(\"Close<480 AND High < 484.40\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um array com os valores de um consulta\n",
    "val High484 = df.filter($\"High\"===484.40).collect()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GroupBY e Aggregate\n",
    "\n",
    "Other Aggregate Functions             \n",
    "http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Chamando função para carregar dataframe\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregando dataframe Sales.csv\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").\n",
    "csv(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/Sales.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver esquema\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Groupby para volunas categoricas\n",
    "// Opcional, geralmente não salva em outro objeto\n",
    "df.groupBy(\"Company\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo a Média agrupada por Company\n",
    "df.groupBy(\"Company\").mean().show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo a contagem agrupada por Company\n",
    "df.groupBy(\"Company\").count().show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo o maximo agrupado por Company\n",
    "df.groupBy(\"Company\").max().show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo o maximo agrupado por Company\n",
    "df.groupBy(\"Company\").min().show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Pedindo o somatório agrupado por Company\n",
    "df.groupBy(\"Company\").sum().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Contagem de salários distintos\n",
    "df.select(countDistinct(\"Sales\")).show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Somatorio de salários distintos\n",
    "df.select(sumDistinct(\"Sales\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Variância dos salários\n",
    "df.select(variance(\"Sales\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Desvio padrão dos salários\n",
    "df.select(stddev(\"Sales\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Coletando salarios\n",
    "df.select(collect_set(\"Sales\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ordenando de forma crescente\n",
    "df.orderBy(\"Sales\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ordenando de forma decrescente\n",
    "df.orderBy($\"Sales\".desc).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing Data\n",
    "\n",
    "\n",
    "Você basicamente tem 3 opções com valores nulos                 \n",
    "\n",
    "1. Apenas mantê-los, talvez apenas deixe uma certa porcentagem passar\n",
    "2. Descartar valores\n",
    "3. Preencha-os com algum outro valor (média, mediana, interpolação ou imputação com métodos avançados)\n",
    "\n",
    "Sem resposta \"correta\", você terá que se ajustar aos dados!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Chamando função para carregar dataframe\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregando dataframe com missings values\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").\n",
    "csv(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/ContainsNull.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver esquema\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Descartando valores\n",
    "df.na.drop().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Elimine quaisquer linhas que tenham menos do que um número mínimo\n",
    "// de valores NÃO nulos (<Int)\n",
    "df.na.drop(2).show ()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Preencha os valores Na com Int\n",
    "df.na.fill(100).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Preencher String irá apenas para todas as colunas de string\n",
    "df.na.fill(\"Emp Name Missing\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Seja mais específico, passe uma matriz de nomes de coluna de string\n",
    "df.na.fill(\"Specific\",Array(\"Name\")).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Preencher Vendas com vendas médias.\n",
    "df.describe().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Agora preencha com os valores da média \n",
    "df.na.fill(400.5).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dates e Hora\n",
    "\n",
    "Ver mais informações em:              \n",
    "http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$@add_months(startDate:org.apache.spark.sql.Column,numMonths:Int):org.apache.spark.sql.Column\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Chamando função para carregar dataframe\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregando dataframe CitiGroup2006_2008\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").\n",
    "csv(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/CitiGroup2006_2008.csv\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver o esquema\n",
    "df.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "df.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Selecionando o mês da coluna Date\n",
    "df.select(month(df(\"Date\"))).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Selecionando o ano da coluna Date\n",
    "df.select(year(df(\"Date\"))).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um data frame e criar uma coluna somente com o Ano\n",
    "val df2 = df.withColumn(\"Year\",year(df(\"Date\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "df2.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Média por ano da variável Close\n",
    "val dfavgs = df2.groupBy(\"Year\").mean()\n",
    "dfavgs.select($\"Year\",$\"avg(Close)\").show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Minímo por ano da variável Close\n",
    "val dfmins = df2.groupBy(\"Year\").min()\n",
    "dfmins.select($\"Year\",$\"min(Close)\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine Learning com SPARK / SCALA\n",
    "\n",
    "Mais informações em:        \n",
    "https://spark.apache.org/docs/latest/ml-guide.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regresssão com Spark\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html                    \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**EXEMPLO 1**: \n",
    "\n",
    "O objetivo neste exemplo entender como rodar uma regressão linear e pedir os coeficientes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Iniciando uma sessão do Spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importando módulos de regressão linear e para construir a base de dados\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Para ver menos avisos\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Preparando a base de treinamento e teste.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"multiline\", \"true\").\n",
    "format(\"csv\").load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/USA_Housing.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Checando os dados\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Veja um exemplo de como os dados se parecem\n",
    "// imprimindo uma linha\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Preparando DataFrame para Machine Learning**\n",
    "\n",
    "* Algumas coisas que precisamos fazer antes que o Spark possa aceitar os dados!\n",
    "* Precisa ter a forma de duas colunas (\"rótulo\", \"recursos\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Isso nos permitirá juntar várias colunas de recursos em uma única coluna de uma matriz de valores feautre\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Renomear Preço para coluna de rótulo.\n",
    "// Pegar apenas colunas numéricas dos dados\n",
    "val df = (data.select(data(\"Price\").as(\"label\"),$\"Avg Area Income\",$\"Avg Area House Age\", $\"Avg Area Number of Rooms\"\n",
    "                                              ,$\"Avg Area Number of Bedrooms\",$\"Area Population\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Checando os dados\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converter os valores de entrada em um vetor que é o que o algoritmo de ML lê para treinar um modelo\n",
    "// Definir as colunas de entrada das quais devemos ler os valores\n",
    "// Definir o nome da coluna onde o vetor será armazenado\n",
    "val assembler = (new VectorAssembler()\n",
    "                  .setInputCols(Array(\"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\n",
    "                                      \"Avg Area Number of Bedrooms\",\"Area Population\")).setOutputCol(\"features\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Usar o assembler para transformar nosso DataFrame em duas colunas\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar a base pronta para rodar a regressão\n",
    "output.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um objeto de modelo de regressão linear\n",
    "val lr = new LinearRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Rodar a regressão \n",
    "val lrModel = lr.fit(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Imprime os coeficientes e o intercepto da regressão linear\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Summarize the model over the training set and print out some metrics!\n",
    "// Explore this in the spark-shell for more methods to call\n",
    "val trainingSummary = lrModel.summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar Número de iterações de descida gradiente a serem executadas.\n",
    "println(s\"numIterations: ${trainingSummary.totalIterations}\")\n",
    "println(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar RMSE, MSE e R² \n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar coeficientes, valor da estatisticas T e P-valor\n",
    "println(s\"Coefficient Standard Errors: ${trainingSummary.coefficientStandardErrors.mkString(\",\")}\")\n",
    "println(s\"T Values: ${trainingSummary.tValues.mkString(\",\")}\")\n",
    "println(s\"P Values: ${trainingSummary.pValues.mkString(\",\")}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar valores previstos\n",
    "trainingSummary.predictions.show()\n",
    "\n",
    "// Mostrar resíduos\n",
    "// Obs: os resíduos são a diferença entre o label e o prediction\n",
    "trainingSummary.residuals.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**EXEMPLO 2**:       \n",
    "\n",
    "O objetivo aqui é aplicar os conhecimentos adquiridos acima para resolver um problema de negócio de um e-commerce, e criar um modelo para prever o valor anual gasto por clientes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulo de regressão linear\n",
    "import org.apache.spark.ml.regression.LinearRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Opcional: Use o seguinte código abaixo para definir o relatório de erros\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Inicie uma sessão no spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use a spark para ler no arquivo CSV dos clientes do comércio eletrônico.\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"multiline\", \"true\").\n",
    "format(\"csv\").load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/Ecommerce Customers\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Imprima o esquema do DataFrame\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// imprima uma linha de exemplo, existem várias maneiras de fazer isso, apenas escolha qualquer maneira que você preferir\n",
    "val colnames = data.columns\n",
    "val firstrow = data.head(1)(0)\n",
    "println(\"\\n\")\n",
    "println(\"Example Data Row\")\n",
    "for(ind <- Range(1,colnames.length)){\n",
    "  println(colnames(ind))\n",
    "  println(firstrow(ind))\n",
    "  println(\"\\n\")\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algumas coisas que precisamos fazer antes que a Spark possa aceitar os dados!\n",
    "É preciso estar na forma de duas colunas (\"label\", \"features\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// importar módulos VectorAssembler e Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Renomear a quantidade anual gastada coluna como \"label\", também pegue apenas as colunas numéricas dos dados\n",
    "// e defina tudo isso como um novo datframe chamado df\n",
    "val df = data.select(data(\"Yearly Amount Spent\").as(\"label\"),$\"Avg Session Length\",$\"Time on App\",\n",
    "                                                             $\"Time on Website\",$\"Length of Membership\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um assembler converte os valores de entrada para um vetor isso é necessário para o algoritmo ml treinar um modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use VectorAssembler para converter as colunas de entrada do df Para uma única coluna de saída de uma matriz \n",
    "// chamada \"features\". Defina as colunas de entrada da qual devemos ler os valores e chame de assembler\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Avg Session Length\",\"Time on App\",\"Time on Website\",\n",
    "                                                         \"Length of Membership\")).setOutputCol(\"features\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use o assembler para transformar nosso DataFrame para as duas colunas: label e features\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Crie um objeto de modelo de Regressão linear\n",
    "val lr = new LinearRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Rode o modelo\n",
    "val lrModel = lr.fit(output)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printa os coeficientes e o intercepto da regressão linear\n",
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Imprima algumas métricas! Use o método .summary fora do seu modelo para criar um objeto e chame de trainingsumy.\n",
    "val trainingSummary = lrModel.summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Moste RMSE, MSE, R², coeficiêntes, T-valor e P-valor\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n",
    "println(s\"MSE: ${trainingSummary.meanSquaredError}\")\n",
    "println(s\"r2: ${trainingSummary.r2}\")\n",
    "println(s\"Coefficient Standard Errors: ${trainingSummary.coefficientStandardErrors.mkString(\",\")}\")\n",
    "println(s\"T Values: ${trainingSummary.tValues.mkString(\",\")}\")\n",
    "println(s\"P Values: ${trainingSummary.pValues.mkString(\",\")}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar valores previstos\n",
    "trainingSummary.predictions.show()\n",
    "\n",
    "// Mostrar resíduos\n",
    "// Obs: os resíduos são a diferença entre o label e o prediction\n",
    "trainingSummary.residuals.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classificação com Spark\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**EXEMPLO 1**: Regressção Logistica                \n",
    "\n",
    "Esse exemplo utiliza a base de dados do kaagle titanic, para prever quem irá sobreviver.\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Iniciar uma Sessão no Spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importando módulo de regressão logistica\n",
    "import org.apache.spark.ml.classification.LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Redefinir o relatório de erros\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregar base de dados\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"multiline\", \"true\").\n",
    "format(\"csv\").load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/titanic.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Imprima o esquema do DataFrame\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show(1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Selecionando apenas as colunas que vamos utilizar\n",
    "val logregdataall = data.select(data(\"Survived\").as(\"label\"), $\"Pclass\", $\"Sex\", $\"Age\", $\"SibSp\",\n",
    "                                                              $\"Parch\", $\"Fare\", $\"Embarked\")\n",
    "\n",
    "// Remover missings\n",
    "val logregdata = logregdataall.na.drop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe logregdata\n",
    "logregdata.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar os módulos VectorAssembler e Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Encodificar as variáveis categóricas strings em númericas\n",
    "val genderIndexer = new StringIndexer().setInputCol(\"Sex\").setOutputCol(\"SexIndex\")\n",
    "val embarkIndexer = new StringIndexer().setInputCol(\"Embarked\").setOutputCol(\"EmbarkIndex\")\n",
    "\n",
    "// Gerar variáveis dummies a partir das varáveis categóricas numéricas\n",
    "val genderEncoder = new OneHotEncoder().setInputCol(\"SexIndex\").setOutputCol(\"SexVec\")\n",
    "val embarkEncoder = new OneHotEncoder().setInputCol(\"EmbarkIndex\").setOutputCol(\"EmbarkVec\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Monte tudo junto para ter o formato (\"label\", \"features\")\n",
    "val assembler = (new VectorAssembler()\n",
    "                  .setInputCols(Array(\"Pclass\", \"SexVec\", \"Age\",\"SibSp\",\"Parch\",\"Fare\",\"EmbarkVec\"))\n",
    "                  .setOutputCol(\"features\") )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Divindo base de dados em treino e teste\n",
    "val Array(training, test) = logregdata.randomSplit(Array(0.7, 0.3), seed = 12345)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostrar a base pronta para rodar a regressão\n",
    "training.show()\n",
    "test.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar o pipeline\n",
    "import org.apache.spark.ml.Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Crie um objeto de modelo de Regressão logistica\n",
    "val lr = new LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar um pipeline. \n",
    "// O pipeline vai garantir que os dados passe por todo os estágios do processo de preparação dos dados que fizemos.\n",
    "val pipeline = new Pipeline().setStages(Array(genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,assembler, lr))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ajuste o pipeline ao treinamento.\n",
    "val model = pipeline.fit(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Obter resultados no conjunto de teste\n",
    "val results = model.transform(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Avaliação a performace do modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulo para ver as métricas de avaliação\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// É necessário converter para RDD para usar este\n",
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Instanciar o objeto de métricas\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerar matriz de confusão\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Métricas do modelo         \n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver acuracia do modelo\n",
    "val accuracy = metrics.accuracy\n",
    "println(\"Summary Statistics\")\n",
    "println(s\"Accuracy = $accuracy\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Precisão pela variável dependente\n",
    "val labels = metrics.labels\n",
    "labels.foreach { l =>\n",
    "  println(s\"Precision($l) = \" + metrics.precision(l))\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Recall por label\n",
    "labels.foreach { l =>\n",
    "  println(s\"Recall($l) = \" + metrics.recall(l))\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Taxa de falsos positivos por label\n",
    "labels.foreach { l =>\n",
    "  println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// F-measure por label\n",
    "labels.foreach { l =>\n",
    "  println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Estatísticas ponderadas\n",
    "println(s\"Weighted precision: ${metrics.weightedPrecision}\")\n",
    "println(s\"Weighted recall: ${metrics.weightedRecall}\")\n",
    "println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n",
    "println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "EXEMPLO 2: Regressção Logistica.\n",
    "\n",
    "Neste exemplo é utilizado uma base de dados de publicidade falsa que indica se um usuário da internet clicou em um anúncio. \n",
    "O objetivo é criar um modelo capaz de prever se os usuários clicarão em um anúncio.\n",
    "\n",
    "\n",
    "Este conjunto de dados contém as seguintes variáveis:\n",
    "* Daily Time Spent on Site: Tempo diário gasto no site em minutos\n",
    "* Age:  idade do cutomer em anos\n",
    "* Area Income: Média de Renda da área geográfica do consumidor\n",
    "* Daily Internet Usage: Média em minutos por dia o consumidor fica na internet\n",
    "* Ad Topic Line: Título do anúncio\n",
    "* City: Cidade do consumidor\n",
    "* Male: se o consumidor é homem ou não\n",
    "* Country': País do consumidor\n",
    "* Timestamp: hora em que o consumidor clicou no anúncio ou fechou a janela\n",
    "* Clicked on Ad: 0 ou 1 indicado clicando no anúncio\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar e criar uma sessão no spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar Logisitic Regression\n",
    "import org.apache.spark.ml.classification.LogisticRegression"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Redefinir o relatório de erros\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregar base de dados\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"multiline\", \"true\")\n",
    ".format(\"csv\").load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/advertising.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar esquema do DataFrame\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show(1000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar coluna Hour\n",
    "val timedata = data.withColumn(\"Hour\",hour(data(\"Timestamp\")))\n",
    "\n",
    "// Selecionar colunas do label\n",
    "val logregdata = (timedata.select(data(\"Clicked on Ad\").as(\"label\"),\n",
    "                    $\"Daily Time Spent on Site\", $\"Age\", $\"Area Income\",$\"Daily Internet Usage\",$\"Hour\",$\"Male\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar VectorAssembler e Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um novo VectorAssembler chamado assembler para o feature\n",
    "val assembler = (new VectorAssembler().setInputCols(Array(\"Daily Time Spent on Site\", \"Age\", \"Area Income\",\n",
    "          \"Daily Internet Usage\",\"Hour\")).setOutputCol(\"features\"))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Divindo base de dados em treino e teste\n",
    "val Array(training, test) = logregdata.randomSplit(Array(0.7, 0.3), seed = 1234)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar Pipeline\n",
    "import org.apache.spark.ml.Pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar uma novo objeto LogisticRegression chamado lr\n",
    "val lr = new LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um novo pipeline com estaágios: assembler, lr\n",
    "val pipeline = new Pipeline().setStages(Array(assembler, lr))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Rodando a regressão logistica\n",
    "val model = pipeline.fit(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Gerando os resultados na base de teste \n",
    "val results = model.transform(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulos de metricas para avaliação do modelo\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Converta os resultados do teste em um RDD usando .as e .rdd\n",
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Instancie um novo objeto MulticlassMetrics\n",
    "val metrics = new MulticlassMetrics(predictionAndLabels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Imprima a matriz de confusão\n",
    "println(\"Confusion matrix:\")\n",
    "println(metrics.confusionMatrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Avaliação de modelos de machine learning\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "EXEMPLO 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulos necessários\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregar base de dados\n",
    "val data = spark.read.format(\"libsvm\")\n",
    ".load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes//sample_linear_regression_data.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar esquema do DataFrame\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Dividir base em treino e teste\n",
    "val Array(training, test) = data.randomSplit(Array(0.9, 0.1), seed = 12345)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Usamos um ParamGridBuilder para construir uma grade de parâmetros a serem pesquisados.\n",
    "// TrainValidationSplit tentará todas as combinações de valores e determinará o melhor modelo usando o avaliador.\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01))\n",
    ".addGrid(lr.fitIntercept).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).build()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar objetio lr \n",
    "val lr = new LinearRegression().setMaxIter(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Neste caso, o estimador é simplesmente a regressão linear.\n",
    "// Um TrainValidationSplit requer um Estimator, um conjunto de Estimator ParamMaps e um Evaluator.\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(lr)\n",
    "  .setEvaluator(new RegressionEvaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  // 80% será usado para treino e 20% para teste\n",
    "  .setTrainRatio(0.8)\n",
    "  // Avalie até 2 configurações de parâmetro em paralelo\n",
    "  .setParallelism(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Execute a divisão de validação do trem e escolha o melhor conjunto de parâmetros.\n",
    "val model = trainValidationSplit.fit(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Faça previsões sobre os dados de teste. modelo é o modelo com combinação de parâmetros aquele teve o melhor desempenho.\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// ver o melhor modelo\n",
    "model.bestModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// ver os prâmetros dos modelos\n",
    "model.getEstimatorParamMaps\n",
    "\n",
    "// OBS: consultar a documentação para ver melhor a seleção dos melhores modelos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "EXEMPLO 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Começar uma sessão no spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Impotar módulos\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Impotar módulos\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Carregar base de dados\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").option(\"multiline\", \"true\")\n",
    ".format(\"csv\").load(\"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/USA_Housing.csv\")\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Preparar dados para machine learning\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Renomear coluna de rótulo\n",
    "// Pega apenas colunas numéricas\n",
    "val df = data.select(data(\"Price\").as(\"label\"),$\"Avg Area Income\",$\"Avg Area House Age\",\n",
    "                     $\"Avg Area Number of Rooms\",$\"Area Population\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Um assembler converte os valores de entrada em um vetor\n",
    "// Um vetor é o que o algoritmo de ML lê para treinar um modelo\n",
    "\n",
    "// Defina as colunas de entrada das quais devemos ler os valores\n",
    "// Defina o nome da coluna onde o vetor será armazenado\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\n",
    "    \"Avg Area Income\",\"Avg Area House Age\",\"Avg Area Number of Rooms\",\"Area Population\")).setOutputCol(\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// tranformar DataFrame\n",
    "val output = assembler.transform(df).select($\"label\",$\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um array de treino e teste\n",
    "val Array(training, test) = output.select(\"label\",\"features\").randomSplit(Array(0.7, 0.3), seed = 12345)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar objeto com a função de regressão linear\n",
    "val lr = new LinearRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// grid de parametros\n",
    "val paramGrid = new ParamGridBuilder().addGrid(lr.regParam,Array(1000,0.001)).build()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Um TrainValidationSplit requer um Estimator, um conjunto de Estimator ParamMaps e um Evaluator.\n",
    "// 80% dos dados serão usados para treinamento e os 20% restantes para validação.\n",
    "val trainValidationSplit = (new TrainValidationSplit()\n",
    "                            .setEstimator(lr)\n",
    "                            .setEvaluator(new RegressionEvaluator())\n",
    "                            .setEstimatorParamMaps(paramGrid)\n",
    "                            .setTrainRatio(0.8))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Você pode então tratar este objeto como o novo modelo e usar o ajuste nele.\n",
    "// Execute a divisão de validação do trem e escolha o melhor conjunto de parâmetros.\n",
    "val model = trainValidationSplit.fit(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Faça previsões sobre os dados de teste. modelo é o modelo com combinação de parâmetros\n",
    "// aquele teve o melhor desempenho.\n",
    "model.transform(test).select(\"features\", \"label\", \"prediction\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Checar as métricas\n",
    "model.validationMetrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clustering com Spark\n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-clustering.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**K-Means Clustering**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Começar uma sessão no spark\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// configuração de alertas\n",
    "import org.apache.log4j._\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulo de clustering\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Loads data.\n",
    "val dataset = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\n",
    "    \"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/Wholesale customers data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "dataset.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Selecione as seguintes colunas para o conjunto de treinamento:\n",
    "// Fresco, Leite, Comestível, Congelado, Detergentes_Paper, Delicassen\n",
    "// Calcular este novo subconjunto feature_data\n",
    "val feature_data = dataset.select($\"Fresh\", $\"Milk\", $\"Grocery\", $\"Frozen\", $\"Detergents_Paper\", $\"Delicassen\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importe VectorAssembler e Vectors\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,VectorIndexer,OneHotEncoder}\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Crie um novo objeto VectorAssembler chamado assembler para o recurso\n",
    "// colunas como a entrada Defina a coluna de saída a ser chamada de recursos\n",
    "// Lembre-se de que não há coluna Label\n",
    "val assembler = new VectorAssembler().setInputCols(Array(\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \n",
    "                                                         \"Detergents_Paper\", \"Delicassen\")).setOutputCol(\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use o objeto assembler para transformar o feature_data\n",
    "// Chame este novo dado training_data\n",
    "val training_data = assembler.transform(feature_data).select(\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Criar um modelo Kmeans com K=3\n",
    "val kmeans = new KMeans().setK(3).setSeed(1L)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Rodar esse modelo ao training_data\n",
    "val model = kmeans.fit(training_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Avalie o clustering calculando dentro do conjunto de soma dos erros quadrados.\n",
    "val WSSSE = model.computeCost(training_data)\n",
    "println(s\"Within Set Sum of Squared Errors = $WSSSE\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Mostre os resultados\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Componentes Principais (PCA)           \n",
    "\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-feature-extraction.html#pca"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar Spark e Criar uma sessão\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"PCA_Example\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Abrir base de dados\n",
    "val data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\n",
    "    \"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/Cancer_Data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Printar esquema dos dados\n",
    "data.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "data.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar PCA, VectorAssembler e StandardScaler do ml.feature\n",
    "import org.apache.spark.ml.feature.{PCA,StandardScaler,VectorAssembler}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar Vectors do ml.linalg\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use VectorAssembler para converter as colunas de entrada dos dados de cânce para uma única coluna de saída de \n",
    "// uma matriz chamada \"recursos\". Defina as colunas de entrada das quais devemos ler os valores.\n",
    "// Chame este novo montador de objeto."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Como existem muitas colunas, você pode achar esta linha útil para apenas passar para setInputCols\n",
    "val colnames = (Array(\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\",\n",
    "\"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\",\n",
    "\"radius error\", \"texture error\", \"perimeter error\", \"area error\", \"smoothness error\", \"compactness error\",\n",
    "\"concavity error\", \"concave points error\", \"symmetry error\", \"fractal dimension error\", \"worst radius\",\n",
    "\"worst texture\", \"worst perimeter\", \"worst area\", \"worst smoothness\", \"worst compactness\", \"worst concavity\",\n",
    "\"worst concave points\", \"worst symmetry\", \"worst fractal dimension\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val assembler = new VectorAssembler().setInputCols(colnames).setOutputCol(\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use o assembler para transformar o dataframe para uma só coluna: features\n",
    "val output = assembler.transform(data).select($\"features\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Freqüentemente, é uma boa ideia normalizar cada recurso para ter um padrão de unidade\n",
    "// desvio e / ou média zero, v ao usar PCA.\n",
    "\n",
    "// Use StandardScaler nos dados. Crie um novo objeto StandardScaler () chamado scaler\n",
    "// Defina a entrada para a coluna de recursos e a saída para uma coluna chamada\n",
    "// scaledFeatures"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val scaler = (new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Calcule as estatísticas de resumo ajustando o StandardScaler.\n",
    "// Basicamente, crie um novo objeto chamado scalerModel usando scaler.fit () na saída do VectorAssembler\n",
    "val scalerModel = scaler.fit(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Normaliza cada recurso para ter um desvio padrão da unidade.\n",
    "// Use transform () fora deste objeto scalerModel para criar seu scaledData\n",
    "val scaledData = scalerModel.transform(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Agora é hora de usar o PCA para reduzir os recursos a alguns componentes principais\n",
    "\n",
    "// Criar um novo objeto PCA () que terá os scaledFeatures e produzir os recursos do pcs, use 4 componentes principais\n",
    "// Em seguida, ajuste isso ao scaledData\n",
    "\n",
    "val pca = (new PCA()\n",
    "  .setInputCol(\"scaledFeatures\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(4)\n",
    "  .fit(scaledData))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Uma vez que seu pca foi criado e ajustado, transforme o scaledData\n",
    "// Chame este novo dataframe de pcaDF\n",
    "val pcaDF = pca.transform(scaledData)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// mostre os novos pcaFeatures\n",
    "val result = pcaDF.select(\"pcaFeatures\")\n",
    "result.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Use .head () para confirmar que sua coluna de saída Array de pcaFeatures\n",
    "// tem apenas 4 componentes principais\n",
    "result.head(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sistema de Recomendação"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar Spark e Criar uma sessão\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().appName(\"PCA_Example\").getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importar módulos\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Abrir base de dados\n",
    "val ratings = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\n",
    "    \"D:/onedrive/Documentos/ANALISE_DADOS/SOFTWARE_SCALA_SPARK/CURSOS/UDEMY/spark_dataframes/movie_ratings.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// ver esquema dos dados\n",
    "ratings.head()\n",
    "ratings.printSchema()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver dataframe\n",
    "ratings.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Dividir a base em teste e treino\n",
    "val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Construir o modelo de recomendação usando ALS nos dados de treinamento\n",
    "val als = new ALS()\n",
    "  .setMaxIter(5)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"userId\")\n",
    "  .setItemCol(\"movieId\")\n",
    "  .setRatingCol(\"rating\")\n",
    "val model = als.fit(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Avalar o modelo calculando o erro médio da classificação real\n",
    "val predictions = model.transform(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Importe abs()\n",
    "import org.apache.spark.sql.functions._"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val error = predictions.select(abs($\"rating\"-$\"prediction\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver erros\n",
    "error.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver descritivas dos erros\n",
    "error.describe().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Ver descritivas dos erros sem NaNs\n",
    "error.na.drop().describe().show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Referências \n",
    "\n",
    "1 - https://www.tutorialspoint.com/scala/index.htm\n",
    "\n",
    "2 - https://www.udemy.com/course/scala-and-spark-for-big-data-and-machine-learning"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "34.9858px",
    "width": "182.997px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}